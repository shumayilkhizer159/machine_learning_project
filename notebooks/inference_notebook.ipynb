{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "print(\"=== SYSTEM DIAGNOSTIC ===\")\n",
        "print(\"Listing ALL files in /kaggle/input:\")\n",
        "if os.path.exists('/kaggle/input'):\n",
        "    found_any = False\n",
        "    for root, dirs, files in os.walk('/kaggle/input'):\n",
        "        level = root.replace('/kaggle/input', '').count(os.sep)\n",
        "        indent = ' ' * 4 * (level)\n",
        "        print(f\"{indent}{os.path.basename(root)}/\")\n",
        "        subindent = ' ' * 4 * (level + 1)\n",
        "        for f in files:\n",
        "            found_any = True\n",
        "            print(f\"{subindent}{f}\")\n",
        "    if not found_any:\n",
        "        print(\"  (Directory is empty)\")\n",
        "else:\n",
        "    print(\"/kaggle/input does not exist (Are you running locally?)\")\n",
        "print(\"=========================\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "import joblib\n",
        "import json\n",
        "import sys\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration\n",
        "CONFIG = {\n",
        "    # 1. COMPETITION DATA (Where test.csv lives)\n",
        "    # DO NOT CHANGE this if you added the 'NFL Big Data Bowl 2026' dataset.\n",
        "    'data_dir': '/kaggle/input/nfl-big-data-bowl-2026-prediction',\n",
        "\n",
        "    # 2. YOUR MODELS (Where you uploaded your 'models' folder)\n",
        "    # CHANGE THIS to match the path of your uploaded dataset.\n",
        "    # It usually looks like: '/kaggle/input/YOUR-DATASET-NAME/models/xgboost'\n",
        "    'models_dir': '/kaggle/input/my-data-set-2/models/xgboost',\n",
        "\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "}\n",
        "\n",
        "# Auto-detect models_dir if not found\n",
        "if not Path(CONFIG['models_dir']).exists():\n",
        "    print(f\"\u26a0\ufe0f Default models_dir '{CONFIG['models_dir']}' not found. Searching...\")\n",
        "    # Search for a known model file to locate the directory\n",
        "    possible_models = list(Path('/kaggle/input').glob('**/xgboost_params.json'))\n",
        "    if possible_models:\n",
        "        # Found the params file, the parent is likely the models dir\n",
        "        # Check if it is inside 'xgboost' folder or if it IS the root\n",
        "        # We expect structure: .../models/xgboost/xgboost_params.json\n",
        "        found_dir = possible_models[0].parent\n",
        "        CONFIG['models_dir'] = str(found_dir)\n",
        "        print(f\"\u2705 Auto-detected models_dir at: {CONFIG['models_dir']}\")\n",
        "    else:\n",
        "        print(\"\u274c Could not auto-detect models directory. Please check 'models_dir' path.\")\n",
        "\n",
        "# Auto-detect competition data if not found\n",
        "if not Path(CONFIG['data_dir']).exists():\n",
        "    print(f\"\u26a0\ufe0f Default data_dir '{CONFIG['data_dir']}' not found. Searching...\")\n",
        "    found_data = False\n",
        "    if Path('/kaggle/input').exists():\n",
        "        for d in Path('/kaggle/input').iterdir():\n",
        "            if d.is_dir() and (d / 'test.csv').exists():\n",
        "                CONFIG['data_dir'] = str(d)\n",
        "                print(f\"\u2705 Auto-detected competition data at: {CONFIG['data_dir']}\")\n",
        "                found_data = True\n",
        "                break\n",
        "    if not found_data:\n",
        "        print(\"\u274c Could not auto-detect competition data. Please check 'Add Data'.\")\n",
        "\n",
        "# Feature columns (Must match training)\n",
        "FEATURE_COLS = [\n",
        "    'x', 'y', 's', 'a', 'dir', 'o',\n",
        "    'ball_land_x', 'ball_land_y', 'dist_to_ball_land',\n",
        "    'v_x', 'v_y',\n",
        "    'player_position_encoded', 'player_side_encoded', 'player_role_encoded',\n",
        "    'player_height_inches', 'player_weight', 'player_age',\n",
        "    'play_direction_binary', 'absolute_yardline_number'\n",
        "]\n",
        "\n",
        "\n",
        "# Verify paths immediately\n",
        "print('Checking configuration paths...')\n",
        "for key, path in CONFIG.items():\n",
        "    if key.endswith('_dir'):\n",
        "        if Path(path).exists():\n",
        "            print(f'\u2705 {key} found: {path}')\n",
        "        else:\n",
        "            print(f'\u274c {key} NOT found: {path}')\n",
        "\n",
        "print('Cell 1 (Imports and Config) executed successfully')\n",
        "\n",
        "\"\"\"\n",
        "Data Loading and Preprocessing Module for NFL Big Data Bowl 2026\n",
        "This module handles loading, cleaning, and preprocessing of NFL tracking data.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from typing import Tuple, List, Dict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "class NFLDataLoader:\n",
        "    \"\"\"\n",
        "    Handles loading and preprocessing of NFL tracking data.\n",
        "    \n",
        "    The data consists of:\n",
        "    - Input files: Player tracking data BEFORE the pass is thrown\n",
        "    - Output files: Player positions AFTER the pass (targets to predict)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, data_dir: str = '/kaggle/input/nfl-big-data-bowl-2026-prediction'):\n",
        "        \"\"\"\n",
        "        Initialize the data loader.\n",
        "        \n",
        "        Args:\n",
        "            data_dir: Path to the directory containing the competition data\n",
        "        \"\"\"\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.train_dir = self.data_dir / 'train'\n",
        "        \n",
        "    def load_week_data(self, week: int) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Load input and output data for a specific week.\n",
        "        \n",
        "        Args:\n",
        "            week: Week number (1-18)\n",
        "            \n",
        "        Returns:\n",
        "            Tuple of (input_df, output_df)\n",
        "        \"\"\"\n",
        "        input_file = self.train_dir / f'input_2023_w{week:02d}.csv'\n",
        "        output_file = self.train_dir / f'output_2023_w{week:02d}.csv'\n",
        "        \n",
        "        input_df = pd.read_csv(input_file)\n",
        "        output_df = pd.read_csv(output_file)\n",
        "        \n",
        "        return input_df, output_df\n",
        "    \n",
        "    def load_all_training_data(self, weeks: List[int] = None) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Load and concatenate data from multiple weeks.\n",
        "        \n",
        "        Args:\n",
        "            weeks: List of week numbers to load. If None, loads all weeks 1-18.\n",
        "            \n",
        "        Returns:\n",
        "            Tuple of (combined_input_df, combined_output_df)\n",
        "        \"\"\"\n",
        "        if weeks is None:\n",
        "            weeks = range(1, 19)  # Weeks 1-18\n",
        "        \n",
        "        input_dfs = []\n",
        "        output_dfs = []\n",
        "        \n",
        "        print(f\"Loading data from {len(weeks)} weeks...\")\n",
        "        for week in weeks:\n",
        "            try:\n",
        "                input_df, output_df = self.load_week_data(week)\n",
        "                input_dfs.append(input_df)\n",
        "                output_dfs.append(output_df)\n",
        "                print(f\"  Week {week}: {len(input_df)} input rows, {len(output_df)} output rows\")\n",
        "            except FileNotFoundError:\n",
        "                print(f\"  Week {week}: Files not found, skipping...\")\n",
        "                continue\n",
        "        \n",
        "        combined_input = pd.concat(input_dfs, ignore_index=True)\n",
        "        combined_output = pd.concat(output_dfs, ignore_index=True)\n",
        "        \n",
        "        print(f\"\\nTotal: {len(combined_input)} input rows, {len(combined_output)} output rows\")\n",
        "        return combined_input, combined_output\n",
        "    \n",
        "    def preprocess_input_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Preprocess input tracking data.\n",
        "        \n",
        "        Args:\n",
        "            df: Raw input dataframe\n",
        "            \n",
        "        Returns:\n",
        "            Preprocessed dataframe\n",
        "        \"\"\"\n",
        "        df = df.copy()\n",
        "        \n",
        "        # Convert play_direction to binary (left=0, right=1)\n",
        "        df['play_direction_binary'] = (df['play_direction'] == 'right').astype(int)\n",
        "        \n",
        "        # Parse player height to inches\n",
        "        if 'player_height' in df.columns:\n",
        "            df['player_height_inches'] = df['player_height'].apply(self._height_to_inches)\n",
        "        \n",
        "        # Calculate age from birth date\n",
        "        if 'player_birth_date' in df.columns:\n",
        "            df['player_birth_date'] = pd.to_datetime(df['player_birth_date'], errors='coerce')\n",
        "            df['player_age'] = (pd.Timestamp('2023-01-01') - df['player_birth_date']).dt.days / 365.25\n",
        "        \n",
        "        # Encode categorical variables\n",
        "        df = self._encode_categorical_features(df)\n",
        "        \n",
        "        # Calculate distance to ball landing location\n",
        "        if 'ball_land_x' in df.columns and 'ball_land_y' in df.columns:\n",
        "            df['dist_to_ball_land'] = np.sqrt(\n",
        "                (df['x'] - df['ball_land_x'])**2 + \n",
        "                (df['y'] - df['ball_land_y'])**2\n",
        "            )\n",
        "        \n",
        "        # Calculate velocity components\n",
        "        if 's' in df.columns and 'dir' in df.columns:\n",
        "            df['v_x'] = df['s'] * np.cos(np.radians(df['dir']))\n",
        "            df['v_y'] = df['s'] * np.sin(np.radians(df['dir']))\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def _height_to_inches(self, height_str: str) -> float:\n",
        "        \"\"\"Convert height string (e.g., '6-2') to inches.\"\"\"\n",
        "        try:\n",
        "            if pd.isna(height_str):\n",
        "                return np.nan\n",
        "            feet, inches = height_str.split('-')\n",
        "            return int(feet) * 12 + int(inches)\n",
        "        except:\n",
        "            return np.nan\n",
        "    \n",
        "    def _encode_categorical_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Encode categorical features using label encoding.\"\"\"\n",
        "        categorical_cols = ['player_position', 'player_side', 'player_role']\n",
        "        \n",
        "        for col in categorical_cols:\n",
        "            if col in df.columns:\n",
        "                df[f'{col}_encoded'] = pd.Categorical(df[col]).codes\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def create_play_sequences(self, input_df: pd.DataFrame, output_df: pd.DataFrame) -> Dict:\n",
        "        \"\"\"\n",
        "        Organize data into sequences for each play and player.\n",
        "        \n",
        "        Args:\n",
        "            input_df: Preprocessed input dataframe\n",
        "            output_df: Output dataframe with targets\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary mapping (game_id, play_id, nfl_id) to sequence data\n",
        "        \"\"\"\n",
        "        sequences = {}\n",
        "        \n",
        "        # Group by play and player\n",
        "        for (game_id, play_id, nfl_id), group in input_df.groupby(['game_id', 'play_id', 'nfl_id']):\n",
        "            # Sort by frame_id to ensure temporal order\n",
        "            group = group.sort_values('frame_id')\n",
        "            \n",
        "            # Get corresponding output data\n",
        "            output_mask = (\n",
        "                (output_df['game_id'] == game_id) & \n",
        "                (output_df['play_id'] == play_id) & \n",
        "                (output_df['nfl_id'] == nfl_id)\n",
        "            )\n",
        "            output_group = output_df[output_mask].sort_values('frame_id')\n",
        "            \n",
        "            # Only include if we have both input and output\n",
        "            if len(group) > 0 and len(output_group) > 0:\n",
        "                sequences[(game_id, play_id, nfl_id)] = {\n",
        "                    'input': group,\n",
        "                    'output': output_group,\n",
        "                    'player_to_predict': group['player_to_predict'].iloc[0] if 'player_to_predict' in group.columns else True,\n",
        "                    'num_frames_output': group['num_frames_output'].iloc[0] if 'num_frames_output' in group.columns else len(output_group)\n",
        "                }\n",
        "        \n",
        "        return sequences\n",
        "\n",
        "\n",
        "class FeatureEngineering:\n",
        "    \"\"\"\n",
        "    Advanced feature engineering for NFL tracking data.\n",
        "    \"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def add_temporal_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Add temporal features based on frame sequences.\n",
        "        \n",
        "        Args:\n",
        "            df: Input dataframe with frame_id\n",
        "            \n",
        "        Returns:\n",
        "            Dataframe with additional temporal features\n",
        "        \"\"\"\n",
        "        df = df.copy()\n",
        "        \n",
        "        # Group by play and player\n",
        "        for (game_id, play_id, nfl_id), group in df.groupby(['game_id', 'play_id', 'nfl_id']):\n",
        "            idx = group.index\n",
        "            \n",
        "            # Calculate changes over time (velocity approximation)\n",
        "            if len(group) > 1:\n",
        "                df.loc[idx, 'dx'] = group['x'].diff().fillna(0)\n",
        "                df.loc[idx, 'dy'] = group['y'].diff().fillna(0)\n",
        "                df.loc[idx, 'ds'] = group['s'].diff().fillna(0)\n",
        "                \n",
        "                # Acceleration approximation\n",
        "                df.loc[idx, 'dv_x'] = group['v_x'].diff().fillna(0) if 'v_x' in group.columns else 0\n",
        "                df.loc[idx, 'dv_y'] = group['v_y'].diff().fillna(0) if 'v_y' in group.columns else 0\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    @staticmethod\n",
        "    def add_interaction_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Add features representing player interactions.\n",
        "        \n",
        "        Args:\n",
        "            df: Input dataframe\n",
        "            \n",
        "        Returns:\n",
        "            Dataframe with interaction features\n",
        "        \"\"\"\n",
        "        df = df.copy()\n",
        "        \n",
        "        # For each frame in each play, calculate distances to other players\n",
        "        for (game_id, play_id, frame_id), group in df.groupby(['game_id', 'play_id', 'frame_id']):\n",
        "            idx = group.index\n",
        "            \n",
        "            # Calculate distance to nearest defender/offensive player\n",
        "            if 'player_side' in group.columns:\n",
        "                for i, row in group.iterrows():\n",
        "                    # Distance to nearest opponent\n",
        "                    opponents = group[group['player_side'] != row['player_side']]\n",
        "                    if len(opponents) > 0:\n",
        "                        distances = np.sqrt(\n",
        "                            (opponents['x'] - row['x'])**2 + \n",
        "                            (opponents['y'] - row['y'])**2\n",
        "                        )\n",
        "                        df.loc[i, 'dist_to_nearest_opponent'] = distances.min()\n",
        "                        df.loc[i, 'avg_dist_to_opponents'] = distances.mean()\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    @staticmethod\n",
        "    def add_physics_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Add physics-based features for trajectory prediction.\n",
        "        \n",
        "        Args:\n",
        "            df: Input dataframe\n",
        "            \n",
        "        Returns:\n",
        "            Dataframe with physics features\n",
        "        \"\"\"\n",
        "        df = df.copy()\n",
        "        \n",
        "        # Time to reach ball landing location (assuming constant velocity)\n",
        "        if 'dist_to_ball_land' in df.columns and 's' in df.columns:\n",
        "            df['time_to_ball'] = df['dist_to_ball_land'] / (df['s'] + 1e-6)  # Add small epsilon to avoid division by zero\n",
        "        \n",
        "        # Direction towards ball landing location\n",
        "        if 'ball_land_x' in df.columns and 'ball_land_y' in df.columns:\n",
        "            df['angle_to_ball'] = np.arctan2(\n",
        "                df['ball_land_y'] - df['y'],\n",
        "                df['ball_land_x'] - df['x']\n",
        "            )\n",
        "            \n",
        "            # Angle difference between current direction and ball direction\n",
        "            if 'dir' in df.columns:\n",
        "                df['angle_diff_to_ball'] = np.abs(np.radians(df['dir']) - df['angle_to_ball'])\n",
        "        \n",
        "        return df\n",
        "\n",
        "\n",
        "def prepare_model_data(sequences: Dict, feature_cols: List[str]) -> Tuple[np.ndarray, np.ndarray, List]:\n",
        "    \"\"\"\n",
        "    Prepare data for model training.\n",
        "    \n",
        "    Args:\n",
        "        sequences: Dictionary of play sequences\n",
        "        feature_cols: List of feature column names to use\n",
        "        \n",
        "    Returns:\n",
        "        Tuple of (X, y, sequence_keys) where:\n",
        "        - X: Input features array of shape (n_sequences, n_frames, n_features)\n",
        "        - y: Target array of shape (n_sequences, n_output_frames, 2)\n",
        "        - sequence_keys: List of (game_id, play_id, nfl_id) tuples\n",
        "    \"\"\"\n",
        "    X_list = []\n",
        "    y_list = []\n",
        "    keys_list = []\n",
        "    \n",
        "    for key, seq_data in sequences.items():\n",
        "        # Only include sequences where player should be predicted\n",
        "        if not seq_data['player_to_predict']:\n",
        "            continue\n",
        "        \n",
        "        input_seq = seq_data['input']\n",
        "        output_seq = seq_data['output']\n",
        "        \n",
        "        # Extract features\n",
        "        try:\n",
        "            features = input_seq[feature_cols].values\n",
        "            targets = output_seq[['x', 'y']].values\n",
        "            \n",
        "            X_list.append(features)\n",
        "            y_list.append(targets)\n",
        "            keys_list.append(key)\n",
        "        except KeyError as e:\n",
        "            print(f\"Warning: Missing feature {e} for sequence {key}\")\n",
        "            continue\n",
        "    \n",
        "    return X_list, y_list, keys_list\n",
        "\nprint('Cell 2 (Data Loader) executed successfully')\n\n",
        "\"\"\"\n",
        "XGBoost-based Model for NFL Player Trajectory Prediction\n",
        "This model uses gradient boosting to predict future positions frame by frame.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Tuple, Dict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.metrics import mean_squared_error\n",
        "    XGBOOST_AVAILABLE = True\n",
        "except ImportError:\n",
        "    XGBOOST_AVAILABLE = False\n",
        "    print(\"Warning: XGBoost not available.\")\n",
        "\n",
        "\n",
        "class XGBoostTrajectoryModel:\n",
        "    \"\"\"\n",
        "    XGBoost model for trajectory prediction.\n",
        "    \n",
        "    Strategy: Train separate models for x and y coordinates at each future time step.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, max_future_frames=30, **xgb_params):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            max_future_frames: Maximum number of future frames to predict\n",
        "            xgb_params: Parameters for XGBoost model\n",
        "        \"\"\"\n",
        "        self.max_future_frames = max_future_frames\n",
        "        self.models_x = {}  # Models for x coordinate at each frame\n",
        "        self.models_y = {}  # Models for y coordinate at each frame\n",
        "        \n",
        "        # Default XGBoost parameters\n",
        "        self.xgb_params = {\n",
        "            'objective': 'reg:squarederror',\n",
        "            'max_depth': 6,\n",
        "            'learning_rate': 0.1,\n",
        "            'n_estimators': 100,\n",
        "            'subsample': 0.8,\n",
        "            'colsample_bytree': 0.8,\n",
        "            'random_state': 42,\n",
        "            'n_jobs': -1\n",
        "        }\n",
        "        \n",
        "        # Load optimized parameters if available\n",
        "        # In notebook environment, we skip this auto-loading.\n",
        "        # Parameters will be loaded via load_models() or passed explicitly.\n",
        "        pass\n",
        "\n",
        "                \n",
        "        self.xgb_params.update(xgb_params)\n",
        "    \n",
        "    def create_features_for_frame(self, input_df: pd.DataFrame, \n",
        "                                  frame_offset: int) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Create features for predicting a specific future frame.\n",
        "        \n",
        "        Args:\n",
        "            input_df: Input tracking data\n",
        "            frame_offset: Which future frame to predict (1, 2, 3, ...)\n",
        "            \n",
        "        Returns:\n",
        "            DataFrame with features\n",
        "        \"\"\"\n",
        "        features = input_df.copy()\n",
        "        \n",
        "        # Add frame offset as a feature\n",
        "        features['frame_offset'] = frame_offset\n",
        "        \n",
        "        # Calculate time to target frame (in seconds)\n",
        "        features['time_to_frame'] = frame_offset / 10.0  # 10 fps\n",
        "        \n",
        "        # Predicted position based on constant velocity\n",
        "        if 'v_x' in features.columns and 'v_y' in features.columns:\n",
        "            features['predicted_x_cv'] = features['x'] + features['v_x'] * features['time_to_frame']\n",
        "            features['predicted_y_cv'] = features['y'] + features['v_y'] * features['time_to_frame']\n",
        "        \n",
        "        # Distance and direction to ball at predicted time\n",
        "        if 'ball_land_x' in features.columns:\n",
        "            features['predicted_dist_to_ball'] = np.sqrt(\n",
        "                (features['predicted_x_cv'] - features['ball_land_x'])**2 +\n",
        "                (features['predicted_y_cv'] - features['ball_land_y'])**2\n",
        "            )\n",
        "        \n",
        "        return features\n",
        "    \n",
        "    def prepare_training_data(self, input_df: pd.DataFrame, \n",
        "                            output_df: pd.DataFrame,\n",
        "                            feature_cols: List[str]) -> Dict[int, Tuple]:\n",
        "        \"\"\"\n",
        "        Prepare training data for each future frame.\n",
        "        \n",
        "        Args:\n",
        "            input_df: Input tracking data\n",
        "            output_df: Output tracking data with targets\n",
        "            feature_cols: List of feature columns\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary mapping frame_offset to (X, y_x, y_y) tuples\n",
        "        \"\"\"\n",
        "        data_by_frame = {}\n",
        "        \n",
        "        # Group by play and player\n",
        "        # Group by play and player\n",
        "        # Optimize: Pre-group output_df to avoid repeated filtering\n",
        "        output_groups = dict(list(output_df.groupby(['game_id', 'play_id', 'nfl_id'])))\n",
        "        \n",
        "        for (game_id, play_id, nfl_id), input_group in input_df.groupby(['game_id', 'play_id', 'nfl_id']):\n",
        "            # Only include players to predict\n",
        "            if not input_group['player_to_predict'].iloc[0]:\n",
        "                continue\n",
        "            \n",
        "            # Get last input frame\n",
        "            last_input = input_group.sort_values('frame_id').iloc[-1]\n",
        "            \n",
        "            # Get output frames from pre-grouped dict\n",
        "            if (game_id, play_id, nfl_id) not in output_groups:\n",
        "                continue\n",
        "                \n",
        "            output_group = output_groups[(game_id, play_id, nfl_id)].sort_values('frame_id')\n",
        "            \n",
        "            if len(output_group) == 0:\n",
        "                continue\n",
        "            \n",
        "            # For each output frame\n",
        "            for i, (_, output_row) in enumerate(output_group.iterrows()):\n",
        "                frame_offset = i + 1  # 1-indexed\n",
        "                \n",
        "                if frame_offset > self.max_future_frames:\n",
        "                    break\n",
        "                \n",
        "                # Create features\n",
        "                features_df = self.create_features_for_frame(\n",
        "                    pd.DataFrame([last_input]), frame_offset\n",
        "                )\n",
        "                \n",
        "                # Extract feature values\n",
        "                try:\n",
        "                    X_row = features_df[feature_cols].values[0]\n",
        "                    y_x = output_row['x']\n",
        "                    y_y = output_row['y']\n",
        "                    \n",
        "                    if frame_offset not in data_by_frame:\n",
        "                        data_by_frame[frame_offset] = {'X': [], 'y_x': [], 'y_y': []}\n",
        "                    \n",
        "                    data_by_frame[frame_offset]['X'].append(X_row)\n",
        "                    data_by_frame[frame_offset]['y_x'].append(y_x)\n",
        "                    data_by_frame[frame_offset]['y_y'].append(y_y)\n",
        "                except KeyError as e:\n",
        "                    continue\n",
        "        \n",
        "        # Convert to arrays\n",
        "        result = {}\n",
        "        for frame_offset, data in data_by_frame.items():\n",
        "            result[frame_offset] = (\n",
        "                np.array(data['X']),\n",
        "                np.array(data['y_x']),\n",
        "                np.array(data['y_y'])\n",
        "            )\n",
        "        \n",
        "        return result\n",
        "    \n",
        "    def train(self, training_data: Dict[int, Tuple], verbose=True):\n",
        "        \"\"\"\n",
        "        Train XGBoost models for each future frame.\n",
        "        \n",
        "        Args:\n",
        "            training_data: Dictionary from prepare_training_data\n",
        "            verbose: Whether to print progress\n",
        "        \"\"\"\n",
        "        if verbose:\n",
        "            print(f\"Training XGBoost models for {len(training_data)} future frames...\")\n",
        "        \n",
        "        for frame_offset, (X, y_x, y_y) in training_data.items():\n",
        "            if verbose:\n",
        "                print(f\"  Frame +{frame_offset}: {len(X)} samples\")\n",
        "            \n",
        "            # Split data\n",
        "            X_train, X_val, y_x_train, y_x_val = train_test_split(\n",
        "                X, y_x, test_size=0.2, random_state=42\n",
        "            )\n",
        "            _, _, y_y_train, y_y_val = train_test_split(\n",
        "                X, y_y, test_size=0.2, random_state=42\n",
        "            )\n",
        "            \n",
        "            # Train model for x coordinate\n",
        "            model_x = xgb.XGBRegressor(**self.xgb_params)\n",
        "            model_x.fit(\n",
        "                X_train, y_x_train,\n",
        "                eval_set=[(X_val, y_x_val)],\n",
        "                verbose=False\n",
        "            )\n",
        "            self.models_x[frame_offset] = model_x\n",
        "            \n",
        "            # Train model for y coordinate\n",
        "            model_y = xgb.XGBRegressor(**self.xgb_params)\n",
        "            model_y.fit(\n",
        "                X_train, y_y_train,\n",
        "                eval_set=[(X_val, y_y_val)],\n",
        "                verbose=False\n",
        "            )\n",
        "            self.models_y[frame_offset] = model_y\n",
        "            \n",
        "            if verbose:\n",
        "                # Calculate validation RMSE\n",
        "                pred_x = model_x.predict(X_val)\n",
        "                pred_y = model_y.predict(X_val)\n",
        "                rmse = np.sqrt(((pred_x - y_x_val)**2 + (pred_y - y_y_val)**2).mean() / 2)\n",
        "                print(f\"    Validation RMSE: {rmse:.4f}\")\n",
        "        \n",
        "        if verbose:\n",
        "            print(\"Training complete!\")\n",
        "    \n",
        "    def predict(self, input_df: pd.DataFrame, feature_cols: List[str],\n",
        "               num_frames: int) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Predict future positions (Single Player).\n",
        "        Wrapper around predict_batch for compatibility.\n",
        "        \"\"\"\n",
        "        # Ensure input is a DataFrame with one row (last frame)\n",
        "        if len(input_df) > 1:\n",
        "             last_input = input_df.sort_values('frame_id').iloc[[-1]]\n",
        "        else:\n",
        "             last_input = input_df\n",
        "             \n",
        "        # Add dummy index for batch processing\n",
        "        last_input = last_input.copy()\n",
        "        \n",
        "        # Predict\n",
        "        batch_preds = self.predict_batch(last_input, feature_cols, num_frames)\n",
        "        \n",
        "        # Extract result\n",
        "        key = (last_input['game_id'].iloc[0], last_input['play_id'].iloc[0], last_input['nfl_id'].iloc[0])\n",
        "        return batch_preds.get(key, np.zeros((num_frames, 2)))\n",
        "\n",
        "    def predict_batch(self, last_input_df: pd.DataFrame, feature_cols: List[str],\n",
        "                     num_frames: int) -> Dict[Tuple, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Predict future positions for multiple players simultaneously.\n",
        "        \n",
        "        Args:\n",
        "            last_input_df: DataFrame containing the LAST frame for each player.\n",
        "            feature_cols: List of feature columns\n",
        "            num_frames: Number of frames to predict\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary mapping (game_id, play_id, nfl_id) -> Array of shape (num_frames, 2)\n",
        "        \"\"\"\n",
        "        # Initialize results dictionary\n",
        "        results = {}\n",
        "        keys = []\n",
        "        for _, row in last_input_df.iterrows():\n",
        "            key = (row['game_id'], row['play_id'], row['nfl_id'])\n",
        "            results[key] = np.zeros((num_frames, 2))\n",
        "            keys.append(key)\n",
        "            \n",
        "        # We will update a working DataFrame frame by frame\n",
        "        current_df = last_input_df.copy()\n",
        "        \n",
        "        # Pre-calculate constant velocity components if available\n",
        "        if 'v_x' in current_df.columns and 'v_y' in current_df.columns:\n",
        "            v_x = current_df['v_x'].values\n",
        "            v_y = current_df['v_y'].values\n",
        "        else:\n",
        "            v_x = np.zeros(len(current_df))\n",
        "            v_y = np.zeros(len(current_df))\n",
        "            \n",
        "        start_x = current_df['x'].values\n",
        "        start_y = current_df['y'].values\n",
        "        \n",
        "        for frame_offset in range(1, num_frames + 1):\n",
        "            # Create features for this batch\n",
        "            # We use the helper but need to ensure it handles batches\n",
        "            features_df = self.create_features_for_frame(current_df, frame_offset)\n",
        "            \n",
        "            X = features_df[feature_cols].values\n",
        "            \n",
        "            # Predict\n",
        "            if frame_offset in self.models_x and frame_offset in self.models_y:\n",
        "                pred_x = self.models_x[frame_offset].predict(X)\n",
        "                pred_y = self.models_y[frame_offset].predict(X)\n",
        "            else:\n",
        "                # Fallback: Constant Velocity\n",
        "                time_delta = frame_offset / 10.0\n",
        "                pred_x = start_x + v_x * time_delta\n",
        "                pred_y = start_y + v_y * time_delta\n",
        "            \n",
        "            # Clip\n",
        "            pred_x = np.clip(pred_x, 0, 120)\n",
        "            pred_y = np.clip(pred_y, 0, 53.3)\n",
        "            \n",
        "            # Store predictions\n",
        "            try:\n",
        "                # Ensure predictions are arrays\n",
        "                if not isinstance(pred_x, np.ndarray):\n",
        "                    pred_x = np.array(pred_x)\n",
        "                if not isinstance(pred_y, np.ndarray):\n",
        "                    pred_y = np.array(pred_y)\n",
        "                    \n",
        "                for i, key in enumerate(keys):\n",
        "                    # Defensive casting to int\n",
        "                    idx_frame = int(frame_offset - 1)\n",
        "                    idx_player = int(i)\n",
        "                    \n",
        "                    results[key][idx_frame] = [pred_x[idx_player], pred_y[idx_player]]\n",
        "            except Exception as e:\n",
        "                print(f\"ERROR in loop: {e}\")\n",
        "                print(f\"frame_offset: {frame_offset}, type: {type(frame_offset)}\")\n",
        "                print(f\"i: {i}, type: {type(i)}\")\n",
        "                print(f\"pred_x type: {type(pred_x)}\")\n",
        "                print(f\"pred_x shape: {getattr(pred_x, 'shape', 'N/A')}\")\n",
        "                # Don't raise, just skip this frame/player to keep running\n",
        "                continue\n",
        "                \n",
        "        # Apply smoothing to all trajectories\n",
        "        for key in results:\n",
        "            results[key] = self.smooth_trajectory(results[key])\n",
        "            \n",
        "        return results\n",
        "    \n",
        "    def smooth_trajectory(self, trajectory: np.ndarray, window_size=5) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Apply smoothing to the predicted trajectory.\n",
        "        Uses a simple moving average.\n",
        "        \"\"\"\n",
        "        if len(trajectory) < window_size:\n",
        "            return trajectory\n",
        "            \n",
        "        smoothed = trajectory.copy()\n",
        "        \n",
        "        # Simple Moving Average for x and y\n",
        "        for i in range(2):  # x and y\n",
        "            # Use pandas rolling mean for convenience if available, else numpy\n",
        "            series = pd.Series(trajectory[:, i])\n",
        "            # Min_periods=1 ensures we don't get NaNs at the start\n",
        "            smoothed[:, i] = series.rolling(window=window_size, min_periods=1, center=True).mean().values\n",
        "            \n",
        "        return smoothed\n",
        "    \n",
        "    def save_models(self, save_dir: str):\n",
        "        \"\"\"Save trained models.\"\"\"\n",
        "        import pickle\n",
        "        from pathlib import Path\n",
        "        \n",
        "        save_dir = Path(save_dir)\n",
        "        save_dir.mkdir(exist_ok=True, parents=True)\n",
        "        \n",
        "        # Save models\n",
        "        for frame_offset, model in self.models_x.items():\n",
        "            model.save_model(save_dir / f'xgb_x_frame_{frame_offset}.json')\n",
        "        \n",
        "        for frame_offset, model in self.models_y.items():\n",
        "            model.save_model(save_dir / f'xgb_y_frame_{frame_offset}.json')\n",
        "        \n",
        "        # Save metadata\n",
        "        metadata = {\n",
        "            'max_future_frames': self.max_future_frames,\n",
        "            'xgb_params': self.xgb_params,\n",
        "            'trained_frames': list(self.models_x.keys())\n",
        "        }\n",
        "        with open(save_dir / 'metadata.pkl', 'wb') as f:\n",
        "            pickle.dump(metadata, f)\n",
        "    \n",
        "    def load_models(self, save_dir: str):\n",
        "        \"\"\"Load trained models.\"\"\"\n",
        "        import pickle\n",
        "        from pathlib import Path\n",
        "        \n",
        "        save_dir = Path(save_dir)\n",
        "        \n",
        "        # Load metadata\n",
        "        with open(save_dir / 'metadata.pkl', 'rb') as f:\n",
        "            metadata = pickle.load(f)\n",
        "        \n",
        "        self.max_future_frames = metadata['max_future_frames']\n",
        "        self.xgb_params = metadata['xgb_params']\n",
        "        \n",
        "        # Load models\n",
        "        for frame_offset in metadata['trained_frames']:\n",
        "            model_x = xgb.XGBRegressor()\n",
        "            model_x.load_model(save_dir / f'xgb_x_frame_{frame_offset}.json')\n",
        "            self.models_x[frame_offset] = model_x\n",
        "            \n",
        "            model_y = xgb.XGBRegressor()\n",
        "            model_y.load_model(save_dir / f'xgb_y_frame_{frame_offset}.json')\n",
        "            self.models_y[frame_offset] = model_y\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if not XGBOOST_AVAILABLE:\n",
        "        print(\"XGBoost is required. Please install: pip install xgboost\")\n",
        "    else:\n",
        "        print(\"XGBoost model module loaded successfully\")\n",
        "\nprint('Cell 3 (XGBoost Model) executed successfully')\n\n",
        "import polars as pl\n",
        "import kaggle_evaluation.nfl_inference_server\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# --- Global Setup ---\n",
        "print('Initializing Global Resources...')\n",
        "\n",
        "# 1. Robust Scaler Search\n",
        "scaler = None\n",
        "print('Searching for scaler.pkl...')\n",
        "possible_scalers = glob.glob('/kaggle/input/**/scaler.pkl', recursive=True)\n",
        "if possible_scalers:\n",
        "    scaler_path = possible_scalers[0]\n",
        "    print(f'\u2705 Found scaler at: {scaler_path}')\n",
        "    scaler = joblib.load(scaler_path)\n",
        "else:\n",
        "    print('\u274c Scaler NOT found. Model predictions will be garbage.')\n",
        "\n",
        "# 2. Load Models\n",
        "model = None\n",
        "print('Searching for metadata.pkl...')\n",
        "possible_models = glob.glob('/kaggle/input/**/metadata.pkl', recursive=True)\n",
        "if possible_models:\n",
        "    model_dir = os.path.dirname(possible_models[0])\n",
        "    print(f'\u2705 Found model directory at: {model_dir}')\n",
        "    loader = NFLDataLoader(CONFIG['data_dir'])\n",
        "    fe = FeatureEngineering()\n",
        "    model = XGBoostTrajectoryModel()\n",
        "    model.load_models(model_dir)\n",
        "    print('\u2705 Models loaded successfully')\n",
        "else:\n",
        "    print('\u274c Model metadata NOT found in /kaggle/input. Model predictions will be garbage.')\n",
        "\n",
        "def predict(test: pl.DataFrame, test_input: pl.DataFrame):\n",
        "    # Initialize with Center Field (Better than 0,0)\n",
        "    n_rows = len(test)\n",
        "    x_preds = [60.0] * n_rows\n",
        "    y_preds = [26.65] * n_rows\n",
        "    \n",
        "    try:\n",
        "        # Convert to Pandas\n",
        "        test_pd = test.to_pandas()\n",
        "        test_input_pd = test_input.to_pandas()\n",
        "        \n",
        "        # DEBUG: Print columns to diagnose 'frame_id' error\n",
        "        if not hasattr(predict, 'debug_printed'):\n",
        "            print(f'DEBUG: Test Columns: {test_pd.columns.tolist()}')\n",
        "            print(f'DEBUG: Input Columns: {test_input_pd.columns.tolist()}')\n",
        "            predict.debug_printed = True\n",
        "\n",
        "        # --- 1. Physics Calculation (Vectorized) ---\n",
        "        # Get last known state for each player\n",
        "        last_input = test_input_pd.sort_values('frame_id').groupby(['game_id', 'play_id', 'nfl_id']).last().reset_index()\n",
        "        \n",
        "        # Calculate velocities\n",
        "        s_filled = last_input['s'].fillna(0.0)\n",
        "        dir_filled = last_input['dir'].fillna(0.0)\n",
        "        last_input['v_x_calc'] = s_filled * np.sin(np.radians(dir_filled))\n",
        "        last_input['v_y_calc'] = s_filled * np.cos(np.radians(dir_filled))\n",
        "        \n",
        "        # Prepare last state for merge\n",
        "        last_input_merge = last_input[['game_id', 'play_id', 'nfl_id', 'x', 'y', 'frame_id', 'v_x_calc', 'v_y_calc']].rename(\n",
        "            columns={'x': 'x_last', 'y': 'y_last', 'frame_id': 'frame_id_last'}\n",
        "        )\n",
        "        \n",
        "        # Merge last state into test dataframe\n",
        "        test_pd['original_index'] = test_pd.index\n",
        "        test_merged = test_pd.merge(\n",
        "            last_input_merge,\n",
        "            on=['game_id', 'play_id', 'nfl_id'],\n",
        "            how='left'\n",
        "        )\n",
        "        \n",
        "        # Calculate dt\n",
        "        sort_cols = ['game_id', 'play_id', 'nfl_id']\n",
        "        if 'frame_id' in test_merged.columns:\n",
        "            sort_cols.append('frame_id')\n",
        "        test_merged = test_merged.sort_values(sort_cols)\n",
        "        test_merged['frame_offset'] = test_merged.groupby(['game_id', 'play_id', 'nfl_id']).cumcount() + 1\n",
        "        test_merged['dt'] = test_merged['frame_offset'] * 0.1\n",
        "            \n",
        "        # Physics Predictions\n",
        "        test_merged['x_cv'] = test_merged['x_last'] + test_merged['v_x_calc'] * test_merged['dt']\n",
        "        test_merged['y_cv'] = test_merged['y_last'] + test_merged['v_y_calc'] * test_merged['dt']\n",
        "        \n",
        "        # Fill NaNs with Center Field\n",
        "        test_merged['x_cv'] = test_merged['x_cv'].fillna(60.0)\n",
        "        test_merged['y_cv'] = test_merged['y_cv'].fillna(26.65)\n",
        "        \n",
        "        # Default final predictions to Physics\n",
        "        test_merged['x_final'] = test_merged['x_cv']\n",
        "        test_merged['y_final'] = test_merged['y_cv']\n",
        "        \n",
        "        # --- 2. XGBoost Prediction ---\n",
        "        if model is not None and scaler is not None:\n",
        "            # Preprocessing\n",
        "            test_input_pd = loader.preprocess_input_data(test_input_pd)\n",
        "            test_input_pd = fe.add_physics_features(test_input_pd)\n",
        "            test_input_pd = fe.add_temporal_features(test_input_pd)\n",
        "            \n",
        "            # Scaling\n",
        "            for col in FEATURE_COLS:\n",
        "                if col not in test_input_pd.columns:\n",
        "                    test_input_pd[col] = 0.0\n",
        "            test_input_pd[FEATURE_COLS] = scaler.transform(test_input_pd[FEATURE_COLS])\n",
        "\n",
        "            # Predict\n",
        "            last_input_preprocessed = test_input_pd.sort_values('frame_id').groupby(['game_id', 'play_id', 'nfl_id']).last().reset_index()\n",
        "            players_to_predict = last_input_preprocessed[last_input_preprocessed['player_to_predict'] == True]\n",
        "            \n",
        "            if len(players_to_predict) > 0:\n",
        "                max_frame = int(test_merged['frame_offset'].max())\n",
        "                batch_preds = model.predict_batch(last_input_preprocessed, FEATURE_COLS, max_frame)\n",
        "                \n",
        "                # Convert to DataFrame\n",
        "                pred_data = []\n",
        "                for key, preds in batch_preds.items():\n",
        "                    for i, (px, py) in enumerate(preds):\n",
        "                        pred_data.append(list(key) + [i + 1, px, py])\n",
        "                \n",
        "                if pred_data:\n",
        "                    pred_df = pd.DataFrame(pred_data, columns=['game_id', 'play_id', 'nfl_id', 'frame_offset', 'x_model', 'y_model'])\n",
        "                    \n",
        "                    # Merge model preds\n",
        "                    test_merged = test_merged.merge(\n",
        "                        pred_df,\n",
        "                        on=['game_id', 'play_id', 'nfl_id', 'frame_offset'],\n",
        "                        how='left'\n",
        "                    )\n",
        "\n",
        "                    # --- 3. SANITY CHECK & ENSEMBLE ---\n",
        "                    # Calculate distance between Model and Physics\n",
        "                    test_merged['dist_diff'] = np.sqrt(\n",
        "                        (test_merged['x_model'] - test_merged['x_cv'])**2 + \n",
        "                        (test_merged['y_model'] - test_merged['y_cv'])**2\n",
        "                    )\n",
        "                    \n",
        "                    # Logic: Aggressive Model Trust (Break the Physics Baseline)\n",
        "                    # Threshold: 15.0 yards (Allow turns)\n",
        "                    # Weights: 0.6 Model / 0.4 Physics\n",
        "                    \n",
        "                    mask_use_ensemble = (test_merged['x_model'].notna()) & (test_merged['dist_diff'] < 15.0)\n",
        "                    \n",
        "                    # Apply Ensemble (0.6 * Model + 0.4 * Physics)\n",
        "                    test_merged.loc[mask_use_ensemble, 'x_final'] = (\n",
        "                        0.6 * test_merged.loc[mask_use_ensemble, 'x_model'] + \n",
        "                        0.4 * test_merged.loc[mask_use_ensemble, 'x_cv']\n",
        "                    )\n",
        "                    test_merged.loc[mask_use_ensemble, 'y_final'] = (\n",
        "                        0.6 * test_merged.loc[mask_use_ensemble, 'y_model'] + \n",
        "                        0.4 * test_merged.loc[mask_use_ensemble, 'y_cv']\n",
        "                    )\n",
        "                    # Else: Keep Physics (already in x_final)\n",
        "                    \n",
        "                    # --- 4. TEMPORAL SMOOTHING ---\n",
        "                    # Smooth the final trajectory to remove jitter\n",
        "                    # Window 3\n",
        "                    test_merged['x_final'] = test_merged.groupby(['game_id', 'play_id', 'nfl_id'])['x_final'].transform(\n",
        "                        lambda x: x.rolling(3, min_periods=1).mean()\n",
        "                    )\n",
        "                    test_merged['y_final'] = test_merged.groupby(['game_id', 'play_id', 'nfl_id'])['y_final'].transform(\n",
        "                        lambda x: x.rolling(3, min_periods=1).mean()\n",
        "                    )\n",
        "\n",
        "        test_merged = test_merged.sort_values('original_index')\n",
        "        x_preds = test_merged['x_final'].values\n",
        "        y_preds = test_merged['y_final'].values\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'CRITICAL ERROR in predict function: {e}')\n",
        "        # Fallback is already initialized to Center Field\n",
        "    return pl.DataFrame({'x': x_preds, 'y': y_preds})\n",
        "\n",
        "# --- Server Startup ---\n",
        "inference_server = kaggle_evaluation.nfl_inference_server.NFLInferenceServer(predict)\n",
        "\n",
        "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
        "    print('Starting Inference Server...')\n",
        "    inference_server.serve()\n",
        "else:\n",
        "    print('Running Local Gateway...')\n",
        "    inference_server.run_local_gateway((CONFIG['data_dir'],))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}